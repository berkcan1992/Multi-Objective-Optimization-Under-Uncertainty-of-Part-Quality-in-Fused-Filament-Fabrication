{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berkc\\Miniconda3\\envs\\Research_AM_2020\\lib\\site-packages\\torch\\serialization.py:420: UserWarning: Couldn't retrieve source code for container of type MC_Dropout_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#    This file is part of DEAP.\n",
    "#\n",
    "#    DEAP is free software: you can redistribute it and/or modify\n",
    "#    it under the terms of the GNU Lesser General Public License as\n",
    "#    published by the Free Software Foundation, either version 3 of\n",
    "#    the License, or (at your option) any later version.\n",
    "#\n",
    "#    DEAP is distributed in the hope that it will be useful,\n",
    "#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
    "#    GNU Lesser General Public License for more details.\n",
    "#\n",
    "#    You should have received a copy of the GNU Lesser General Public\n",
    "#    License along with DEAP. If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "import array, copy, random, time\n",
    "# import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "seaborn.set(style='whitegrid')\n",
    "seaborn.set_context('notebook')\n",
    "\n",
    "# imports for the BNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "from deap import algorithms, base, creator, tools\n",
    "\n",
    "\n",
    "# load min and max values of the data to denormalize prediction data\n",
    "with open('maxmin.pickle', 'rb') as f:\n",
    "    [max_x, min_x, max_y, min_y] = pickle.load(f)\n",
    "\n",
    "def normalize_max_min(data, data_max, data_min):\n",
    "    return (data-data_min) / (data_max-data_min)\n",
    "\n",
    "def denormalize_max_min(data, data_max, data_min):\n",
    "    return data * (data_max-data_min) + data_min\n",
    "\n",
    "class MC_Dropout_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
    "        super(MC_Dropout_Model, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = nn.Linear(input_dim, num_units)\n",
    "        self.layer2 = nn.Linear(num_units, num_units)\n",
    "        self.layer3 = nn.Linear(num_units, 2 * output_dim)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.dropout(x, p=self.drop_prob, training=True)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# # load BL model BNN\n",
    "# toolbox.model = torch.load('BNN_BLmodel.pt')\n",
    "    \n",
    "def evaluate(vars):\n",
    "\n",
    "    # load BL model BNN\n",
    "    BL_model = torch.load('BNN_BLmodel.pt')\n",
    "\n",
    "    max_part_height = 4.2   # maximum part height mm\n",
    "\n",
    "    # number of total layers = (maximum part height)/(height of a layer), i.e., 4.2 / (layer height)\n",
    "    if vars[2] == 1:\n",
    "        height = 0.42\n",
    "    elif vars[2] == 2:\n",
    "        height = 0.6\n",
    "    elif vars[2] == 3:\n",
    "        height = 0.7\n",
    "\n",
    "    num_layers = np.int(max_part_height / height); # number of layers\n",
    "\n",
    "    num_interfaces = 14     # number of interfaces per layer\n",
    "    width = 0.8             # filament width in mm\n",
    "\n",
    "    inp = [] # input to BNN to make predictions\n",
    "    ycoord = 0.5 * height  # 0.5*height of a layer in mm\n",
    "    iki_y = ycoord * 2\n",
    "\n",
    "    # store inputs for GP(model disrepancy at each interface)\n",
    "    for jj in range(1, num_layers + 1):\n",
    "        for ii in range(1, num_interfaces + 1):\n",
    "            # use x & y coordinates of vertical bonds as training data for the GP\n",
    "            # Inp =[ Temperature, speed, height, x, y ]\n",
    "            inp.append([vars[0], vars[1], height, ii * width, ycoord + (jj - 1) * iki_y])\n",
    "\n",
    "    # Convert built Python lists to a Numpy array.\n",
    "    inp = np.array(inp, dtype='float32')\n",
    "\n",
    "    # normalize data\n",
    "    inp = normalize_max_min(inp, max_x, min_x)\n",
    "\n",
    "    x_pred = torch.tensor(inp)  # convert to torch tensor\n",
    "\n",
    "    samples = []\n",
    "    noises = []\n",
    "    for i in range(15):\n",
    "        preds = BL_model.forward(x_pred).cpu().data.numpy()\n",
    "        samples.append(denormalize_max_min(preds[:, 0], max_y, min_y))\n",
    "        noises.append(denormalize_max_min(np.exp(preds[:, 1]), max_y, min_y))\n",
    "\n",
    "    samples, noises = np.array(samples),  np.array(noises)\n",
    "    means = (samples.mean(axis=0)).reshape(-1)\n",
    "\n",
    "    aleatoric = (noises ** 2).mean(axis=0) ** 0.5\n",
    "    epistemic = (samples.var(axis=0) ** 0.5).reshape(-1)\n",
    "    total_unc = (aleatoric ** 2 + epistemic ** 2) ** 0.5\n",
    "\n",
    "    # Dimensionless BL: non-dimensionalize the BL by dividing with the layer height\n",
    "    dimensionless_mean_bl = means.mean()/height\n",
    "    dimensionless_total_unc_bl = total_unc.mean()/height**2\n",
    "\n",
    "    return dimensionless_mean_bl, dimensionless_total_unc_bl\n",
    "\n",
    "\n",
    "IND_SIZE = 3\n",
    "N_CYCLES = 1\n",
    "BOUND_LOW, BOUND_UP = [217, 26, 1], [278, 44, 3]\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", array.array, typecode='d', fitness=creator.FitnessMin, n=IND_SIZE)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register(\"attr_temperature\", random.randint, 217, 278)\n",
    "toolbox.register(\"attr_speed\", random.randint, 26, 44)\n",
    "toolbox.register(\"attr_layer\", random.randint, 1, 3)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_temperature,toolbox.attr_speed,toolbox.attr_layer), n=N_CYCLES)\n",
    "\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def checkBounds(min, max):\n",
    "    def decorator(func):\n",
    "        def wrappper(*args, **kargs):\n",
    "            offspring = func(*args, **kargs)\n",
    "            for child in offspring:\n",
    "                for i in range(len(child)):\n",
    "                    if child[i] > max[i]:\n",
    "                        child[i] = max[i]\n",
    "                    elif child[i] < min[i]:\n",
    "                        child[i] = min[i]\n",
    "            return offspring\n",
    "        return wrappper\n",
    "    return decorator\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=1.0/IND_SIZE)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=BOUND_LOW, up=BOUND_UP, indpb=1.0/IND_SIZE)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "# Bounds on the design variables\n",
    "toolbox.decorate(\"mate\", checkBounds([217, 26, 1], [278, 44, 3]))\n",
    "toolbox.decorate(\"mutate\", checkBounds([217, 26, 1], [278, 44, 3]))\n",
    "\n",
    "toolbox.mut_prob = 0.2  # mutation probability\n",
    "\n",
    "def run_ea(toolbox, stats=None, verbose=False):\n",
    "    pop = toolbox.population(n=toolbox.pop_size)\n",
    "    pop = toolbox.select(pop, len(pop))\n",
    "    return algorithms.eaMuPlusLambda(pop, toolbox, mu=toolbox.pop_size, \n",
    "                                     lambda_=toolbox.pop_size, \n",
    "                                     cxpb=1-toolbox.mut_prob,\n",
    "                                     mutpb=toolbox.mut_prob, \n",
    "                                     stats=stats, \n",
    "                                     ngen=toolbox.max_gen, \n",
    "                                     verbose=verbose)\n",
    "\n",
    "\n",
    "stats = tools.Statistics()\n",
    "stats.register(\"pop\", copy.deepcopy)\n",
    "\n",
    "toolbox.pop_size = 50\n",
    "toolbox.max_gen = 50\n",
    "toolbox.mut_prob = 0.2\n",
    "\n",
    "%time pop, logbook = run_ea(toolbox, stats=stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypervolume computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fronts = [tools.sortLogNondominated(pop, k=len(pop), first_front_only=True) \n",
    "          for pop in logbook.select('pop')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference = np.max([np.max([ind.fitness.values for ind in front], axis=0) for front in fronts], axis=0) + [-0.8, 0.8]\n",
    "reference = np.max([np.max([ind.fitness.values for ind in front], axis=0) for front in fronts], axis=0) + [-0.4, 0.]\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deap.benchmarks.tools as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypervols = [bt.hypervolume(front, reference) for front in fronts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(hypervols)\n",
    "plt.title('Hypervolume of the non-dominated fronts')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Hypervolume');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front = np.array([ind.fitness.values for ind in pop])\n",
    "plt.scatter(front[:,0], front[:,1], c=\"b\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating performance indicators\n",
    "- As already mentioned, we need to evaluate the quality of the solutions produced in every execution of the algorithm.\n",
    "- We will use the hypervolumne indicator for that.\n",
    "- Larger hypervolume values are better.\n",
    "- We already filtered each population a leave only the non-dominated individuals.\n",
    "\n",
    "Calculating the reference point: a point that is “worst” than any other individual in every objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_reference(results, epsilon=0.1):\n",
    "#     alldata = np.concatenate(np.concatenate(results.values))\n",
    "#     obj_vals = [toolbox.evaluate(ind) for ind in alldata]\n",
    "#     return np.max(obj_vals, axis=0) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference = calculate_reference(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute hypervolume of the Pareto-optimal fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research_AM_2020",
   "language": "python",
   "name": "research_am_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
